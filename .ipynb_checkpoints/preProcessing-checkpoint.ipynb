{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyrics Sentiment Analysis Preprocessing\n",
    "\n",
    "## Project Overview\n",
    "This notebook implements preprocessing steps for lyrics sentiment analysis using a lexicon-based approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Data Loading Function\n",
    "def load_lyrics_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Load lyrics dataset from a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the CSV file containing lyrics\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Loaded dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Text Preprocessing Functions\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean the text by:\n",
    "    1. Converting to lowercase\n",
    "    2. Removing special characters\n",
    "    3. Removing extra whitespaces\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text to clean\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Cleaned text\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Remove stopwords from the text\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Text with stopwords removed\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"\n",
    "    Lemmatize the text\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Lemmatized text\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_tokens = word_tokenize(text)\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "    return ' '.join(lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Lexicon-based Sentiment Analysis Function\n",
    "def get_sentiment(text):\n",
    "    \"\"\"\n",
    "    Determine sentiment using TextBlob\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Sentiment label (Positive/Negative/Neutral)\n",
    "    \"\"\"\n",
    "    # Use TextBlob for sentiment analysis\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    \n",
    "    if polarity > 0.05:\n",
    "        return 'Positive'\n",
    "    elif polarity < -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Preprocessing Pipeline\n",
    "def preprocess_lyrics(df, lyrics_column='lyrics'):\n",
    "    \"\"\"\n",
    "    Apply preprocessing steps to lyrics\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe with lyrics\n",
    "    lyrics_column : str, optional\n",
    "        Name of the column containing lyrics\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Preprocessed dataframe\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataframe\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    # Apply preprocessing steps\n",
    "    processed_df['cleaned_lyrics'] = processed_df[lyrics_column].apply(clean_text)\n",
    "    processed_df['lyrics_no_stopwords'] = processed_df['cleaned_lyrics'].apply(remove_stopwords)\n",
    "    processed_df['lemmatized_lyrics'] = processed_df['lyrics_no_stopwords'].apply(lemmatize_text)\n",
    "    \n",
    "    # Perform sentiment analysis\n",
    "    processed_df['sentiment'] = processed_df['lemmatized_lyrics'].apply(get_sentiment)\n",
    "    \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualization Functions\n",
    "def plot_sentiment_distribution(df):\n",
    "    \"\"\"\n",
    "    Create a pie chart of sentiment distribution\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Preprocessed dataframe with sentiment\n",
    "    \"\"\"\n",
    "    # Count sentiments\n",
    "    sentiment_counts = df['sentiment'].value_counts()\n",
    "    \n",
    "    # Create pie chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%')\n",
    "    plt.title('Sentiment Distribution in Lyrics')\n",
    "    plt.show()\n",
    "\n",
    "def word_frequency_analysis(df, lyrics_column='lemmatized_lyrics'):\n",
    "    \"\"\"\n",
    "    Perform basic word frequency analysis\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Preprocessed dataframe\n",
    "    lyrics_column : str, optional\n",
    "        Column containing processed lyrics\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Top frequent words\n",
    "    \"\"\"\n",
    "    # Combine all lyrics\n",
    "    all_lyrics = ' '.join(df[lyrics_column])\n",
    "    \n",
    "    # Tokenize and count\n",
    "    word_tokens = word_tokenize(all_lyrics)\n",
    "    word_freq = nltk.FreqDist(word_tokens)\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    freq_df = pd.DataFrame.from_dict(word_freq, orient='index', columns=['Frequency'])\n",
    "    freq_df.index.name = 'Word'\n",
    "    freq_df = freq_df.reset_index().sort_values('Frequency', ascending=False)\n",
    "    \n",
    "    return freq_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Main Execution\n",
    "def main():\n",
    "    # Load the dataset\n",
    "    # Replace 'path/to/your/lyrics_dataset.csv' with actual path\n",
    "    df = load_lyrics_dataset('path/to/your/lyrics_dataset.csv')\n",
    "    \n",
    "    if df is not None:\n",
    "        # Preprocess lyrics\n",
    "        processed_df = preprocess_lyrics(df)\n",
    "        \n",
    "        # Visualize sentiment distribution\n",
    "        plot_sentiment_distribution(processed_df)\n",
    "        \n",
    "        # Perform word frequency analysis\n",
    "        top_words = word_frequency_analysis(processed_df)\n",
    "        print(\"Top 20 Most Frequent Words:\")\n",
    "        print(top_words)\n",
    "        \n",
    "        # Optional: Save processed dataset\n",
    "        processed_df.to_csv('processed_lyrics_dataset.csv', index=False)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
